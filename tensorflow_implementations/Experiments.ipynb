{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from autoencoders.vanilla_autoencoders import AUTOENCODER_300_150_300\n",
    "from autoencoders.vanilla_autoencoders import AUTOENCODER_150\n",
    "from autoencoders.vanilla_autoencoders import AUTOENCODER_50\n",
    "\n",
    "\n",
    "from home.pn.PycharmProjects.autoencoders.helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, m = get_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784-300-150-300-784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experimental_tasks1 = []\n",
    "settings = [(5, 0.0001), (20, 0.0001), (50, 0.0001)]\n",
    "#train_data, test_data, m = get_mnist_data()\n",
    "\n",
    "for (num_epochs, l2_reg) in settings:\n",
    "\n",
    "    model = AUTOENCODER_300_150_300()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #Train\n",
    "    train_loss, test_loss = [], []\n",
    "    batch_size = 200\n",
    "    with tf.Session()   as sess:\n",
    "        init.run()\n",
    "        train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "        test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "        print(\"Number of Epochs = \" + str(num_epochs))\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch: \" + str(epoch) + \"/\" + str(num_epochs), end=\"\\r\")\n",
    "            n_batches = m//batch_size\n",
    "            for batch in range(n_batches):\n",
    "                X_batch = train_data[batch*batch_size: (batch + 1)*batch_size]\n",
    "                sess.run(model.training_op, feed_dict={model.X: X_batch})\n",
    "            train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "            test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "\n",
    "        reconstructions = model.outputs.eval(feed_dict={model.X: test_data[0: 10]})\n",
    "\n",
    "    experimental_tasks1.append(((num_epochs, l2_reg), train_loss, test_loss))\n",
    "\n",
    "    print(\"Train Loss: \", train_loss[-1])\n",
    "    print(\"Test Loss: \", test_loss[-1])\n",
    "    plot_reconstructions(test_data[0: 10], reconstructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784-150-784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experimental_tasks2 = []\n",
    "settings = [(5, 0.0001), (20, 0.0001), (50, 0.0001)]\n",
    "#train_data, test_data, m = get_mnist_data()\n",
    "\n",
    "for (num_epochs, l2_reg) in settings:\n",
    "\n",
    "    model = AUTOENCODER_150()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #Train\n",
    "    train_loss, test_loss = [], []\n",
    "    batch_size = 200\n",
    "    with tf.Session()   as sess:\n",
    "        init.run()\n",
    "        train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "        test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "        print(\"Number of Epochs = \" + str(num_epochs))\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch: \" + str(epoch) + \"/\" + str(num_epochs), end=\"\\r\")\n",
    "            n_batches = m//batch_size\n",
    "            for batch in range(n_batches):\n",
    "                X_batch = train_data[batch*batch_size: (batch + 1)*batch_size]\n",
    "                sess.run(model.training_op, feed_dict={model.X: X_batch})\n",
    "            train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "            test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "\n",
    "        reconstructions = model.outputs.eval(feed_dict={model.X: test_data[0: 10]})\n",
    "\n",
    "    experimental_tasks2.append(((num_epochs, l2_reg), train_loss, test_loss))\n",
    "\n",
    "    print(\"Train Loss: \", train_loss[-1])\n",
    "    print(\"Test Loss: \", test_loss[-1])\n",
    "    plot_reconstructions(test_data[0: 10], reconstructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784-50-784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experimental_tasks3 = []\n",
    "settings = [(5, 0.0001), (20, 0.0001), (50, 0.0001)]\n",
    "#train_data, test_data, m = get_mnist_data()\n",
    "\n",
    "for (num_epochs, l2_reg) in settings:\n",
    "\n",
    "    model = AUTOENCODER_50()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #Train\n",
    "    train_loss, test_loss = [], []\n",
    "    batch_size = 2000\n",
    "    with tf.Session()   as sess:\n",
    "        init.run()\n",
    "        train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "        test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "        print(\"Number of Epochs = \" + str(num_epochs))\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch: \" + str(epoch) + \"/\" + str(num_epochs), end=\"\\r\")\n",
    "            n_batches = m//batch_size\n",
    "            for batch in range(n_batches):\n",
    "                X_batch = train_data[batch*batch_size: (batch + 1)*batch_size]\n",
    "                sess.run(model.training_op, feed_dict={model.X: X_batch})\n",
    "            train_loss.append(model.loss.eval(session=sess, feed_dict={model.X: train_data}))\n",
    "            test_loss.append(model.loss.eval(session=sess, feed_dict={model.X: test_data}))\n",
    "\n",
    "        reconstructions = model.outputs.eval(feed_dict={model.X: test_data[0: 10]})\n",
    "\n",
    "    experimental_tasks3.append(((num_epochs, l2_reg), train_loss, test_loss))\n",
    "\n",
    "    print(\"Train Loss: \", train_loss[-1])\n",
    "    print(\"Test Loss: \", test_loss[-1])\n",
    "    plot_reconstructions(test_data[0: 10], reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([experimental_tasks1, experimental_tasks2, experimental_tasks3])\n",
    "#print(experimental_tasks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
